{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "from torch import optim\n",
    "from matplotlib.patches import FancyArrow\n",
    "import matplotlib.cm as cm\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import time\n",
    "import keyboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.multiprocessing as mp\n",
    "from data_loader_script import create_data_loader\n",
    "\n",
    "mp.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers):\n",
    "        super(Encoder,self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, dropout= 0.0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.fc1(x)\n",
    "        out , _ = self.lstm(y)\n",
    "        return out\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, num_heads):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first= True, dropout= 0.0)\n",
    "        self.attn = nn.MultiheadAttention(hidden_dim, num_heads, dropout= 0.0, batch_first= True)\n",
    "\n",
    "    def forward(self, x, enc_outs, h0, c0, indices_to_ignore):\n",
    "        # LSTM output\n",
    "        y, (hn, cn) = self.lstm(x, (h0, c0))  # y: (N, 1, d)\n",
    "\n",
    "        # Create a mask for attention\n",
    "        # enc_outs: (N, L, d), indices_to_ignore: (N, s)\n",
    "        N, L, _ = enc_outs.shape\n",
    "        mask = torch.zeros((N, L), dtype=torch.bool, device=enc_outs.device)  # Initialize mask to False\n",
    "\n",
    "        # Set True for indices to ignore\n",
    "        if not indices_to_ignore is None:\n",
    "            for i in range(N):\n",
    "                mask[i, indices_to_ignore[i]] = True\n",
    "\n",
    "        # Apply attention with mask\n",
    "        _, attn_weights = self.attn(query=y, key=enc_outs, value=enc_outs, key_padding_mask=mask)  # Masked attention\n",
    "        attn_weights = attn_weights.squeeze(1)  # (N, L)\n",
    "\n",
    "        return (hn, cn), attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TSPNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, Num_L_enc =3, \n",
    "                Num_L_dec = 3, num_heads = 2):\n",
    "        super(TSPNet, self).__init__()\n",
    "        self.Num_L_dec = Num_L_dec\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.encoder = Encoder(input_dim, hidden_dim, Num_L_enc)\n",
    "        self.decoder = Decoder(hidden_dim, hidden_dim, Num_L_dec, num_heads)\n",
    "    def forward(self, X, mod = 'train'):\n",
    "\n",
    "        batch_size, seq_length, _ = X.size()\n",
    "\n",
    "        encoded_cities = self.encoder(X) # output shape: (batch_size, num_cities, hidden_dim)\n",
    "\n",
    "        h0,c0 = torch.zeros(self.Num_L_dec, batch_size, self.hidden_dim).to(device), torch.zeros(self.Num_L_dec, batch_size, self.hidden_dim).to(device)\n",
    "        #indices_to_ignore = torch.cat((torch.zeros(batch_size,1),torch.zeros(batch_size,1)-1),dim=-1).long()\n",
    "        \n",
    "        start_token = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n",
    "        x_star = start_token.expand(batch_size, -1, -1).to(device)# the first input to the decoder is a vector we have to learn\n",
    "\n",
    "        outs = torch.zeros(batch_size, seq_length+1, seq_length).to(device)\n",
    "        action_indices = torch.zeros(batch_size, seq_length+1, 1).to(device)\n",
    "\n",
    "        indices_to_ignore = None # for the first input, we can visit all the cities.\n",
    "        \n",
    "        for t in range(seq_length+1):\n",
    "            if t == seq_length:\n",
    "                indices_to_ignore = indices_to_ignore[:,1:]\n",
    "            (hn,cn), attn_weights = self.decoder(x_star, encoded_cities, h0,c0, indices_to_ignore)\n",
    "            attn_weights = torch.clamp(attn_weights, min=1e-9)\n",
    "            attn_weights = attn_weights / attn_weights.sum(dim=-1, keepdim=True)\n",
    "            if mod == 'train':\n",
    "                idx = torch.multinomial(attn_weights, num_samples=1).squeeze(-1)\n",
    "            elif mod == 'eval':\n",
    "                idx = torch.argmax(attn_weights, dim=-1)\n",
    "            else:\n",
    "                raise('wrong mode')\n",
    "            x_star = encoded_cities[torch.arange(batch_size), idx, :].unsqueeze(1)\n",
    "            outs[:,t,:] = attn_weights\n",
    "            action_indices[:,t,0] = idx\n",
    "            h0,c0 = hn,cn\n",
    "            if t==0:\n",
    "                indices_to_ignore = idx.unsqueeze(-1)\n",
    "            else:\n",
    "                indices_to_ignore = torch.cat((indices_to_ignore, idx.unsqueeze(-1)),dim=-1).long()\n",
    "            \n",
    "        return outs, action_indices\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_cost(cities, routes):\n",
    "    B, N, _ = cities.shape\n",
    "    routes = routes.squeeze(-1).long()  # Convert to long for indexing\n",
    "    ordered_cities = cities[torch.arange(B).unsqueeze(1), routes]  # Reorder cities based on routes\n",
    "    diffs = ordered_cities[:, :-1] - ordered_cities[:, 1:]  # Compute differences between consecutive cities\n",
    "    distances = torch.norm(diffs, p=2, dim=2)  # Euclidean distances\n",
    "    total_distances = distances.sum(dim=1)  # Sum distances for each batch\n",
    "    return total_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def generate_unit_circle_cities(B, N, d):\n",
    "    \"\"\"\n",
    "    Generates a PyTorch tensor of size (B, N, d), representing B batches\n",
    "    of N cities in d-dimensional space, where cities are randomly placed on the unit circle.\n",
    "    \n",
    "    Args:\n",
    "        B (int): Number of batches.\n",
    "        N (int): Number of cities in each batch.\n",
    "        d (int): Number of dimensions (must be at least 2, higher dimensions will have zeros).\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: A tensor of shape (B, N, d) with cities on the unit circle.\n",
    "    \"\"\"\n",
    "    if d < 2:\n",
    "        raise ValueError(\"Dimension 'd' must be at least 2.\")\n",
    "\n",
    "    # Generate random angles for each city\n",
    "    angles = torch.rand(B, N) * 2 * math.pi  # Random angles in radians\n",
    "\n",
    "    # Coordinates on the unit circle\n",
    "    x_coords = torch.cos(angles)\n",
    "    y_coords = torch.sin(angles)\n",
    "\n",
    "    # Create a tensor of zeros for higher dimensions if d > 2\n",
    "    higher_dims = torch.zeros(B, N, d - 2)\n",
    "\n",
    "    # Combine x, y, and higher dimensions\n",
    "    unit_circle_coords = torch.stack((x_coords, y_coords), dim=-1)\n",
    "    result = torch.cat((unit_circle_coords, higher_dims), dim=-1)\n",
    "    result[:,0,:] = result[:,-1,:]\n",
    "    return result\n",
    "cities = generate_unit_circle_cities(10,10,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "================================================================================\n",
       "Layer (type:depth-idx)                                  Param #\n",
       "================================================================================\n",
       "TSPNet                                                  --\n",
       "├─Encoder: 1-1                                          --\n",
       "│    └─Linear: 2-1                                      384\n",
       "│    └─LSTM: 2-2                                        264,192\n",
       "├─Decoder: 1-2                                          --\n",
       "│    └─LSTM: 2-3                                        264,192\n",
       "│    └─MultiheadAttention: 2-4                          49,536\n",
       "│    │    └─NonDynamicallyQuantizableLinear: 3-1        16,512\n",
       "================================================================================\n",
       "Total params: 594,816\n",
       "Trainable params: 594,816\n",
       "Non-trainable params: 0\n",
       "================================================================================"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "input_dim = 2\n",
    "hidden_dim = 128\n",
    "num_layers = 2\n",
    "num_heads = 1\n",
    "model = TSPNet(input_dim, hidden_dim, num_layers, num_layers, num_heads).to(device)\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|\u001b[32m          \u001b[0m| 1/1954 [00:03<1:47:44,  3.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0 Mean cost: 26.12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|\u001b[32m▌         \u001b[0m| 101/1954 [02:21<52:00,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 Mean cost: 26.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|\u001b[32m█         \u001b[0m| 201/1954 [04:47<32:40,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 200 Mean cost: 25.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|\u001b[32m█▌        \u001b[0m| 301/1954 [07:09<46:35,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 300 Mean cost: 26.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|\u001b[32m█▋        \u001b[0m| 334/1954 [07:53<38:18,  1.42s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 22\u001b[0m\n\u001b[0;32m     19\u001b[0m data_batch \u001b[38;5;241m=\u001b[39m data_batch\u001b[38;5;241m.\u001b[39mto(device, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):  \u001b[38;5;66;03m# Mixed precision for speed-up\u001b[39;00m\n\u001b[1;32m---> 22\u001b[0m     outs, actions \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m     sum_log_prob \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(torch\u001b[38;5;241m.\u001b[39mlog(torch\u001b[38;5;241m.\u001b[39mcat([\n\u001b[0;32m     24\u001b[0m         outs[i][torch\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(outs[i])), actions[i]\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\u001b[38;5;241m.\u001b[39mflatten()]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(outs))\n\u001b[0;32m     26\u001b[0m     ], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     27\u001b[0m     costs \u001b[38;5;241m=\u001b[39m route_cost(data_batch, actions)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mTSPNet.forward\u001b[1;34m(self, X, mod)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m==\u001b[39m seq_length:\n\u001b[0;32m     28\u001b[0m     indices_to_ignore \u001b[38;5;241m=\u001b[39m indices_to_ignore[:,\u001b[38;5;241m1\u001b[39m:]\n\u001b[1;32m---> 29\u001b[0m (hn,cn), attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_star\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_cities\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh0\u001b[49m\u001b[43m,\u001b[49m\u001b[43mc0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices_to_ignore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mclamp(attn_weights, \u001b[38;5;28mmin\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-9\u001b[39m)\n\u001b[0;32m     31\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m attn_weights \u001b[38;5;241m/\u001b[39m attn_weights\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\salar\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 30\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[1;34m(self, x, enc_outs, h0, c0, indices_to_ignore)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m indices_to_ignore \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N):\n\u001b[1;32m---> 30\u001b[0m         mask[i, indices_to_ignore[i]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Apply attention with mask\u001b[39;00m\n\u001b[0;32m     33\u001b[0m _, attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(query\u001b[38;5;241m=\u001b[39my, key\u001b[38;5;241m=\u001b[39menc_outs, value\u001b[38;5;241m=\u001b[39menc_outs, key_padding_mask\u001b[38;5;241m=\u001b[39mmask)  \u001b[38;5;66;03m# Masked attention\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "lr = 0.001\n",
    "batch_size = 512\n",
    "num_samples = 1000000\n",
    "num_cities = 50\n",
    "input_dim = 2\n",
    "num_workers = 8  # Start with 1 worker and test scaling up\n",
    "from torch.amp import GradScaler, autocast\n",
    "data_loader = create_data_loader(batch_size, num_samples, num_cities, input_dim, num_workers=num_workers)\n",
    "\n",
    "scaler = GradScaler('cuda')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "alpha = 0.1\n",
    "run_name = 'runs/TSP/' + str(batch_size) + '_' + str(num_cities) + '_' + str(num_samples) + '_' + '/ANN/'+datetime.now().strftime((\"%Y_%m_%d %H_%M_%S\"))\n",
    "writer = SummaryWriter(log_dir=run_name)\n",
    "\n",
    "for episode, data_batch in enumerate(tqdm(data_loader, colour='green')):\n",
    "    data_batch = data_batch.to(device, non_blocking=True)\n",
    "\n",
    "    with autocast('cuda'):  # Mixed precision for speed-up\n",
    "        outs, actions = model(data_batch)\n",
    "        sum_log_prob = torch.sum(torch.log(torch.cat([\n",
    "            outs[i][torch.arange(len(outs[i])), actions[i].cpu().numpy().astype(int).flatten()].unsqueeze(0)\n",
    "            for i in range(len(outs))\n",
    "        ], axis=0)), axis=1).to(device)\n",
    "        costs = route_cost(data_batch, actions).to(device)\n",
    "        \n",
    "        policy_loss = torch.sum(sum_log_prob * costs) / batch_size\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    scaler.scale(policy_loss).backward()  # Scale loss for mixed precision\n",
    "    scaler.step(optimizer)  # Use scaler to handle optimizer step\n",
    "    scaler.update()  # Update scaler for next iteration\n",
    "\n",
    "    # Logging and monitoring\n",
    "    if episode % 100 == 0:\n",
    "        mean_cost = costs.mean().item()\n",
    "        print(f\"Episode: {episode} Mean cost: {mean_cost:.2f}\")\n",
    "        writer.add_scalar('Mean cost', mean_cost, episode)\n",
    "\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(index)  # Minimal implementation\n",
    "\n",
    "dataset = SimpleDataset(1000)\n",
    "data_loader = DataLoader(dataset, batch_size=32, num_workers=1)\n",
    "for data in data_loader:\n",
    "    print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_routes(cities, routes, batch_index=0):\n",
    "    \"\"\"\n",
    "    Plots the route for a given batch of cities and routes.\n",
    "    \n",
    "    Args:\n",
    "        cities (torch.Tensor): Tensor of shape (B, N, 2) representing city coordinates.\n",
    "        routes (torch.Tensor): Tensor of shape (B, N) representing routes.\n",
    "        batch_index (int): Index of the batch to plot.\n",
    "    \"\"\"\n",
    "    cities = cities[batch_index].numpy()\n",
    "    route = routes[batch_index].long().squeeze().numpy()\n",
    "    print(route)\n",
    "    # Get coordinates of cities in the order of the route\n",
    "    ordered_cities = cities[route]\n",
    "    \n",
    "    # Plot cities\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(cities[:, 0], cities[:, 1], color='blue', zorder=2, label='Cities')\n",
    "    for i, (x, y) in enumerate(cities):\n",
    "        plt.text(x, y, f'{i}', fontsize=12, ha='right', color='black')\n",
    "    \n",
    "    # Plot the route\n",
    "    plt.plot(ordered_cities[:, 0], ordered_cities[:, 1], color='red', linestyle='--', zorder=1, label='Route')\n",
    "    \n",
    "    # Highlight start and end points\n",
    "    plt.scatter(ordered_cities[0, 0], ordered_cities[0, 1], color='green', s=100, label='Start', zorder=3)\n",
    "    plt.scatter(ordered_cities[-1, 0], ordered_cities[-1, 1], color='purple', s=100, label='End', zorder=3)\n",
    "    \n",
    "    plt.title(f\"Route for Batch {batch_index}\")\n",
    "    plt.xlabel(\"X Coordinate\")\n",
    "    plt.ylabel(\"Y Coordinate\")\n",
    "\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.001\n",
    "batch_size = 128\n",
    "num_cities = 10\n",
    "N_episodes = 25000\n",
    "data = generate_unit_circle_cities(batch_size, num_cities, input_dim).to(device)\n",
    "_, actions = model(data,mod='eval')\n",
    "plot_routes(data.cpu(),actions.cpu(),11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_city = torch.rand(1, num_cities, input_dim).to(device)\n",
    "_ , actions = model(test_city,mod='eval')\n",
    "plot_routes(test_city.cpu(),actions.cpu(),0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
